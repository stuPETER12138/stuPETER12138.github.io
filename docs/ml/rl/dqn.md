# DQN

DQN 是基于 Q 学习和神经网络的算法，引入了经验回收（experience replay）和目标网络（target network）两种技术。

> [Gymnasium Documentation](https://gymnasium.farama.org/)

## 从监督学习与 Q 学习的区别谈起

在神经网络的训练中，为防止数据偏差，通常可以从数据集中随机取出数据。在Q学习中，每当智能代理对环境采取行动时都会产生数据。具体来说，使用在某个时刻 $t$ 得到的 $E_t=(S_t,A_t,R_t,S_{t+1})$ 更新 Q 函数。这里将 $E_t$ 称为"经验数据"。这个经验数据虽然是随着时间 $t$ 的推移而获得的，但经验数据之间有很强的相关性（例如，$E$ 和 $E_{t+1}$ 之间有很强的相关性）。也就是说，Q学习使用强相关（有偏差）的数据进行训练。这就是监督学习和 Q 学习的第一个区别。弥补这个差异的技术包括“经验回放”。

经验回放也可以用于其他异策略型的强化学习算法。

在监督学习中，我们要为训练数据添加正确答案标签。在这种情况下，输入的正确答案标签不变。以 MNIST 数据集为例，如果输人图像的正确答案标签为"7"，那么该标签将总是为"7"。在神经网络的训练过程中，它的标签当然不会从"7"变成"4"。那么Q学习是怎样的情况呢？Q学习会将 $Q(S_t,A_t)$ 的值更新为 $R_t+\gamma\max_aQ(S_{t+1},a)$，即它会朝着 TD 目标更新 Q 函数。TD 目标相当于监督学习中的正确答案标签。但是，TD 目标的值会随着 Q 函数的更新而变动。这就是监督学习和 Q 学习的区别。为了弥补这种差异，我们使用一种固定 TD 目标的技术，这种技术叫作"目标网络"。

首先，准备一个表示Q函数的原始网络（这个网络叫作 q_net)。然后，再准备了一个具有相同结构的网络（这个网络叫作 q_net_target)。q net 通过正常的 Q 学习进行更新。q_net_target 定期与 q_net 权重同步，其余时间里保持权重参数固定。接下来用 q_net_target 计算 TD 目标的值，就能抑制作为监督标签的 TD 目标的变化了。
