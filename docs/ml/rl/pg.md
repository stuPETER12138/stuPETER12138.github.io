# 策略梯度法

之前的方法中，我们对价值函数（Q 函数或状态价值函数）进行建模并训练，之后借助价值函数获得策略，这样的方法叫做基于价值的方法（value-based method）。而不考虑价值函数直接表示策略的方法，叫做基于策略的方法（policy-based method）。通过神经网络将策略模型化，并通过梯度来优化模型，这种方法叫做策略梯度法（policy gradient method）。

## 策略梯度方法的优点

1. 策略直接模型化，更高效

    基于策略的方法可以直接预测策略。并且有些具有形状复杂的价值函数的问题，其最优策略能很简单。对于这样的情况，基于策略的方法有望更快地训练。

2. 可以用于连续的行动空间

    基于策略的方法对于连续的行动空间的情况也能轻松应对。例如，对于神经网络的输出呈正态分布的情况，可以考虑让神经网络输出正态分布的均值和方差。根据其均值和方差进行采样，即可得到连续值。

3. 行动的选择概略平滑变化

    基于策略的方法是通过 Softmax 函数确定各行动的概率。因此，在更新策略参数的过程中，各行动的概率会平滑地变化。这使得策略梯度法的训练更趋于稳定。

## 统一的范式

策略梯度方法可以用统一的数学式表达。
$$
\nabla_{\theta} \mathcal{J}(\theta) = \mathbb{E}_{\tau \sim \pi_\theta} \left[ \sum^T_{t=0} \Phi_t \nabla_{\theta} \log \pi_{\theta}(A_t | S_t) \right]
$$

- $\Phi = G(\tau)$  最简单的策略梯度法
- $\Phi = G_t$  REINFORCE
- $\Phi = G_t - b(S_t)$  REINFORCE with baseline
- $\Phi = R_t + \gamma V(S_{t+1}) - V(S_t)$  Actor-Critic

## 展开说说

由“状态、行动、奖励”构成的时间序列数据称为轨迹（trajectory）。
$$
\tau = (S_0, A_0, R_0, S_1, A_1, R_1, \cdots , S_{T+1})
$$
此时有带折现率 $\gamma$ 的收益。
$$
G(\tau) = R_0 + \gamma R_1 + \gamma^2 R_2 +\cdots + \gamma^T R_{T}
$$
以此为基础寻找最有函数的办法就是最简单的策略梯度法。而当我们要评估某个时刻 $t$ 做出的行动 $A_t$,只考虑一定时间内（$t \sim T$）的收益 $G_t$ 作为权重,这便是 REINFORCE 算法。

基线（baseline）方法的目的是减小离散数据之间的方差，带来更高样本效率的训练。例如对于某组数据，可以用之前数据的平均值与当前数据取差值，来代替该数据并求得方差。

Actor-Critic 方法是将价值函数模型化，同时训练两个模型。用价值模型 $V_\omega$ 来评估基于策略模型 $\pi_\theta$ 做出行动的好坏。