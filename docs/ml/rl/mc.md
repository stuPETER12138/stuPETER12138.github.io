# 蒙特卡洛方法

强化学习领域主要处理的是在环境模型未知的情况下寻找更好的策略的问题。要做到这一点，智能代理需要实际采取行动并从经验中学习。

蒙特卡洛方法（Monte Carlo method）是对数据进行反复采样并根据结果进行估计的方法的总称，它可以用来从经验中估计价值函数。其中，"经验"是指从环境和智能代理之间的实际互动中获得的数据。具体来说，经验是一系列关于状态、行动和奖励的数据。

## 初识蒙特卡洛方法

我们将一个表示概率分布的模型称为分布模型（distribution model），将只需要采样的模型称为样本模型（sample model）。分布模型要求持有明确的概率分布，而样本模型则要求能够进行采样。与环境相关的模型可以这么分类，而智能代理行动的方式也可以基于分布模型和样本模型来实现。

做大量的采样并取平均值，根据大数定律，该平均值会趋近于正确值。

> 辛钦大数定律：设随机变量 $X_1, \, X_2, \, \cdots, \, X_N, \, \cdots$ 独立同分布，具有数学期望 $\mathbb{E}[X_i] = \mu$，$i = 1, 2, 3, \cdots$，则对任意 $\epsilon > 0$，有
> $$
> \lim_{n\to{\infty}} P \left\{ \left| \frac{1}{n} \sum^n_{i = 1} - \mu \right| < \epsilon \right\} = 1 .
> $$
>

## 评估策略

我们让智能代理根据策略 $\pi$ 采取实际行动。这样得到的实际收益就是样本数据。蒙特卡洛方法会收集大量这样的数据，并计算它们的平均值。用 $G^{(i)}$ 表示第 $i$ 回合获得的收益，价值函数的评估为：
$$
\begin{aligned}
V_n(s) &= \frac{ G^{(1)} + G^{(2)} + \cdots + G^{(n)} }{n} \\
&= V_{n - 1}(s) + \frac{1}{n} \left\{ G^{(n)} - V_{n - 1}(s) \right\}
\end{aligned}
$$
对 Q 函数的评估为：
$$
\begin{aligned}
Q_n(s, a) &= \frac{ G^{(1)} + G^{(2)} + \cdots + G^{(n)} }{n} \\
&= Q_{n - 1}(s, a) + \frac{1}{n} \left\{ G^{(n)} - Q_{n - 1}(s, a) \right\}
\end{aligned}
$$


## 策略控制

得到价值函数后，我们可以利用“贪婪化”，选择价值函数取最大值的行动。这个阶段的数学式如下：
$$
\begin{aligned}
\mu(s) &= \mathop{\arg\max}_a \, Q(s, a) \\
&= \mathop{\arg\max}_a \sum_{s'} p(s' | s,a) \left\{ r(s, a, s') + {\gamma}V(s') \right\}
\end{aligned}
$$
一般的强化学习问题中，我们并不知道环境模型，因此我们往往采用Q函数来改进策略。

## 异策略型和重要性采样

### 同策略型和异策略型

基于从其他地方获得的经验来改进自己的策略，这种做法叫做异策略型（off-policy），而基于自己的经验改进自己的策略的做法叫做同策略型（on-policy）。

> 从作用的角度来看，智能代理的策略可以分为：
>
> - 目标策略（target policy）：一种作为评估和改进对象的策略，负责利用。我们会对这种策略进行评估和改进。
> - 行为策略（behavior policy）：一种智能代理实际用来采取行动的策略，负责探索。智能代理会基于这种策略产生状态、行动和奖励的样本数据。

前面的策略中，我们没有区分目标策略和行为策略。也就是说，之前的"用于评估和改进的目标策略"与"用于实际行动的行为策略"是一样的。这种目标策略和行为策略相同的情况叫作“同策略型”。与之相比，分开考虑目标策略和行为策略的情况叫作“异策略型”。同策略型中的“on”和异策略模型中的“off”在这里分别指的是“连接”和“分开”的意思。异策略型意味着目标策略和行为策略是相互分开的。

例如，我们可以观察网球运动员的挥拍来改善自己的挥拍技术。也就是根据从其他策略（行为策略）中获得的经验来评估和改进自己的策略（目标策略）。通过异策略型，我们可以只让行为策略进行探索，只让目标策略进行利用。而在使用从行为策略获得的样本数据来求目标策略的期望值时，我们需要一些计算上的技巧。此时就需要用到重要性采样（importance sampling）这项技术。

### 重要性采样

重要性采样利用从其他概率分布中采样的数据来计算某个概率分布的期望值。

例如，当 $x$ 是从概率分布 $b$ （而不是 $\pi$）中采样的，有：
$$
\begin{aligned}
\mathbb{E}_{\pi} \left[ x \right] &= \sum x \pi(x) \\
&= \sum x \frac{\pi(x)}{b(x)} b(x) \\
&= \mathbb{E}_{b} \left[ x \frac{\pi(x)}{b(x)} \right]
\end{aligned}
$$
